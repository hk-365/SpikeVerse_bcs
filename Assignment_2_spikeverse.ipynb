{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "g3MLrYsC_CMO"
      },
      "outputs": [],
      "source": [
        "# Step 1: Environment Setup with Gymnasium\n",
        "import ale_py\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import random\n",
        "import pygame\n",
        "from typing import Optional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AtariPreprocessor:\n",
        "    \"\"\"Handles frame preprocessing for Atari environments\"\"\"\n",
        "    def __init__(self, frame_size=84):\n",
        "        self.frame_size = frame_size\n",
        "        self.frame_buffer = deque(maxlen=4)\n",
        "\n",
        "    def preprocess(self, frame):\n",
        "        \"\"\"Convert RGB frame to grayscale and resize\"\"\"\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        resized = cv2.resize(gray, (self.frame_size, self.frame_size))\n",
        "        return resized.astype(np.uint8)\n",
        "\n",
        "    def stack_frames(self, frame, reset=False):\n",
        "        \"\"\"Stack 4 consecutive frames\"\"\"\n",
        "        if reset:\n",
        "            self.frame_buffer.clear()\n",
        "            for _ in range(4):\n",
        "                self.frame_buffer.append(frame)\n",
        "        else:\n",
        "            self.frame_buffer.append(frame)\n",
        "        return np.stack(self.frame_buffer, axis=0)"
      ],
      "metadata": {
        "id": "Nx-gRbf2_Qu8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Custom Gymnasium Wrappers\n",
        "class EpisodicLifeWrapper(gym.Wrapper):\n",
        "    \"\"\"End episode only when all lives are exhausted\"\"\"\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        self.was_real_done = terminated or truncated\n",
        "\n",
        "        current_lives = self.env.unwrapped.ale.lives()\n",
        "        if 0 < current_lives < self.lives:\n",
        "            terminated = False\n",
        "        self.lives = current_lives\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        if self.was_real_done:\n",
        "            obs, info = self.env.reset(**kwargs)\n",
        "            self.lives = self.env.unwrapped.ale.lives()\n",
        "        else:\n",
        "            obs, _, _, _, info = self.env.step(0)\n",
        "        return obs, info\n"
      ],
      "metadata": {
        "id": "8s0zk9_K_UEz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Sum Tree Implementation for Prioritized Replay\n",
        "class SumTree:\n",
        "    \"\"\"Binary heap structure for efficient priority sampling\"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.data = np.empty(capacity, dtype=object)\n",
        "        self.write_ptr = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def _propagate(self, idx, change):\n",
        "        parent = (idx - 1) // 2\n",
        "        self.tree[parent] += change\n",
        "        if parent != 0:\n",
        "            self._propagate(parent, change)\n",
        "\n",
        "    def _retrieve(self, idx, s):\n",
        "        left = 2 * idx + 1\n",
        "        if left >= len(self.tree):\n",
        "            return idx\n",
        "\n",
        "        if s <= self.tree[left]:\n",
        "            return self._retrieve(left, s)\n",
        "        else:\n",
        "            return self._retrieve(left + 1, s - self.tree[left])\n",
        "\n",
        "    def total(self):\n",
        "        return self.tree[0]\n",
        "\n",
        "    def add(self, priority, data):\n",
        "        idx = self.write_ptr + self.capacity - 1\n",
        "        self.data[self.write_ptr] = data\n",
        "        self.update(idx, priority)\n",
        "        self.write_ptr = (self.write_ptr + 1) % self.capacity\n",
        "        self.size = min(self.size + 1, self.capacity)\n",
        "\n",
        "    def update(self, idx, priority):\n",
        "        change = priority - self.tree[idx]\n",
        "        self.tree[idx] = priority\n",
        "        self._propagate(idx, change)\n",
        "\n",
        "    def get(self, s):\n",
        "        idx = self._retrieve(0, s)\n",
        "        data_idx = idx - self.capacity + 1\n",
        "        return (idx, self.tree[idx], self.data[data_idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return current number of stored elements\"\"\"\n",
        "        return self.size\n"
      ],
      "metadata": {
        "id": "IF1JhE6Y_XjV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Prioritized Replay Buffer\n",
        "class PrioritizedReplayBuffer:\n",
        "    \"\"\"Experience buffer with importance sampling\"\"\"\n",
        "    def __init__(self, capacity, alpha=0.6, beta=0.4, initial_size=10000):  # More reasonable initial size\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.initial_size = initial_size\n",
        "        self.tree = SumTree(capacity)\n",
        "        self.max_priority = 1.0\n",
        "        self.epsilon = 1e-6\n",
        "\n",
        "    def add(self, transition):\n",
        "        \"\"\"Add new experience with max priority\"\"\"\n",
        "        priority = self.max_priority ** self.alpha\n",
        "        self.tree.add(priority, transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample batch with importance sampling weights\"\"\"\n",
        "        batch = []\n",
        "        indices = []\n",
        "        priorities = []\n",
        "        segment = self.tree.total() / batch_size\n",
        "\n",
        "        self.beta = min(1.0, self.beta + 0.0001)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "            s = random.uniform(a, b)\n",
        "            idx, priority, data = self.tree.get(s)\n",
        "            priorities.append(priority)\n",
        "            batch.append(data)\n",
        "            indices.append(idx)\n",
        "\n",
        "        probs = np.array(priorities) / self.tree.total()\n",
        "        weights = (len(self.tree) * probs) ** -self.beta\n",
        "        weights /= weights.max()\n",
        "\n",
        "        return batch, indices, np.array(weights, dtype=np.float32)\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        \"\"\"Update priorities after training\"\"\"\n",
        "        priorities = np.abs(priorities) + self.epsilon\n",
        "        priorities = priorities ** self.alpha\n",
        "        for idx, priority in zip(indices, priorities):\n",
        "            self.tree.update(idx, priority)\n",
        "            self.max_priority = max(self.max_priority, priority)\n"
      ],
      "metadata": {
        "id": "5zqbTRoY_bxi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Deep Q-Network Architecture\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"CNN architecture for Atari games\"\"\"\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)\n",
        "\n",
        "\n",
        "config = {\n",
        "    \"gamma\": 0.99,\n",
        "    \"learning_rate\": 2.5e-4,  # More stable LR for Atari DQN\n",
        "    \"batch_size\": 32,\n",
        "    \"epsilon_decay\": 0.9997,\n",
        "    \"epsilon_min\": 0.05,\n",
        "    \"target_update\": 1000,\n",
        "    \"train_step\": 0,\n",
        "    \"epsilon\": 1.0,\n",
        "    \"buffer_capacity\": 100000,\n",
        "    \"episodes\": 2000,\n",
        "    \"learning_starts\": 10000,\n",
        "    \"train_frequency\": 4\n",
        "}\n"
      ],
      "metadata": {
        "id": "1g8j0CKm_gI4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: DQN Agent Implementation\n",
        "class DQNAgent:\n",
        "    \"\"\"Deep Q-Learning Agent with Prioritized Experience Replay\"\"\"\n",
        "    def __init__(self, state_shape, action_size, config):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.action_size = action_size\n",
        "        self.gamma = config['gamma']\n",
        "        self.lr = config['learning_rate']\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.buffer_capacity = config['buffer_capacity']\n",
        "\n",
        "        self.q_net = DQN(state_shape, action_size).to(self.device)\n",
        "        self.target_net = DQN(state_shape, action_size).to(self.device)\n",
        "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=self.lr)\n",
        "        self.buffer = PrioritizedReplayBuffer(self.buffer_capacity)\n",
        "        self.update_target()\n",
        "\n",
        "    def update_target(self):\n",
        "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
        "\n",
        "    def act(self, state, epsilon=0.0):\n",
        "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
        "        if random.random() > epsilon:\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device) / 255.0\n",
        "            with torch.no_grad():\n",
        "                return self.q_net(state).argmax().item()\n",
        "        return random.randint(0, self.action_size-1)\n",
        "\n",
        "    def compute_loss(self, batch, weights):\n",
        "        \"\"\"Prioritized Double DQN loss with importance sampling\"\"\"\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.from_numpy(np.asarray(states, dtype=np.uint8)).float().to(self.device) / 255.0\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.from_numpy(np.asarray(next_states, dtype=np.uint8)).float().to(self.device) / 255.0\n",
        "        dones = torch.BoolTensor(dones).to(self.device)\n",
        "        weights = torch.FloatTensor(weights).to(self.device)\n",
        "\n",
        "        current_q = self.q_net(states).gather(1, actions.unsqueeze(-1))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_actions = self.q_net(next_states).argmax(1)\n",
        "            next_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(-1))\n",
        "            target_q = rewards.unsqueeze(-1) + (1 - dones.float().unsqueeze(-1)) * self.gamma * next_q\n",
        "\n",
        "        td_error = torch.abs(current_q - target_q).squeeze()\n",
        "        loss = (weights * F.smooth_l1_loss(current_q.squeeze(), target_q.squeeze(), reduction='none')).mean()\n",
        "\n",
        "        return loss, td_error.detach().cpu().numpy()\n",
        "\n",
        "    def update(self, batch_size):\n",
        "        if len(self.buffer.tree) < self.buffer.initial_size:\n",
        "            return None\n",
        "\n",
        "        batch, indices, weights = self.buffer.sample(batch_size)\n",
        "        loss, td_errors = self.compute_loss(batch, weights)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 10.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.buffer.update_priorities(indices, td_errors)\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        super().__init__(env)\n",
        "        self._obs_buffer = deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self._obs_buffer.clear()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs, info\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        obs, _, terminated, truncated, _ = self.env.step(1)\n",
        "        if terminated or truncated:\n",
        "            obs, info = self.env.reset(**kwargs)\n",
        "        return obs, info\n"
      ],
      "metadata": {
        "id": "hz3XzVHj_ja2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Training Loop\n",
        "class AtariTrainer:\n",
        "    \"\"\"Complete training pipeline for Atari games\"\"\"\n",
        "    def __init__(self, config):\n",
        "        env_name = \"ALE/Breakout-v5\"\n",
        "        self.env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "        self.preprocessor = AtariPreprocessor()\n",
        "\n",
        "        self.env = MaxAndSkipEnv(self.env, skip=4)\n",
        "        self.env = FireResetEnv(self.env)\n",
        "        self.env = EpisodicLifeWrapper(self.env)\n",
        "        self.config = config\n",
        "        self.best_mean_score = -float('inf')\n",
        "\n",
        "        self.state_shape = (4, 84, 84)\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.agent = DQNAgent(self.state_shape, self.action_size, config)\n",
        "\n",
        "        self.episodes = config['episodes']\n",
        "        self.epsilon = config['epsilon']\n",
        "        self.epsilon_decay = config['epsilon_decay']\n",
        "        self.epsilon_min = config['epsilon_min']\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.target_update = config['target_update']\n",
        "        self.train_step = 0\n",
        "        self.learning_starts = config['learning_starts']\n",
        "        self.train_frequency = config['train_frequency']\n",
        "\n",
        "    def _process_obs(self, obs, reset=False):\n",
        "        \"\"\"Apply preprocessing pipeline\"\"\"\n",
        "        processed = self.preprocessor.preprocess(obs)\n",
        "        return self.preprocessor.stack_frames(processed, reset)\n",
        "\n",
        "    def render_env(self, obs, screen):\n",
        "        frame_rgb = cv2.cvtColor(obs, cv2.COLOR_BGR2RGB)\n",
        "        frame_resized = cv2.resize(frame_rgb, (672, 672))\n",
        "        surface = pygame.surfarray.make_surface(np.transpose(frame_resized, (1, 0, 2)))\n",
        "        screen.fill((0, 0, 0))\n",
        "        screen.blit(surface, (0, 0))\n",
        "        pygame.display.flip()\n",
        "        pygame.time.delay(1000 // 15)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        episode_rewards = []\n",
        "\n",
        "        for episode in range(self.config['episodes']):\n",
        "            obs, _ = self.env.reset()\n",
        "            state = self._process_obs(obs, reset=True)\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "\n",
        "            if (episode + 1) % 100 == 0:\n",
        "                pygame.init()\n",
        "                screen = pygame.display.set_mode((672, 672))\n",
        "                pygame.display.set_caption(f\"Breakout Episode {episode+1}\")\n",
        "                render = True\n",
        "            else:\n",
        "                render = False\n",
        "\n",
        "            while not done:\n",
        "                action = self.agent.act(state, self.epsilon)\n",
        "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                done = terminated or truncated\n",
        "                next_state = self._process_obs(next_obs)\n",
        "\n",
        "                self.agent.buffer.add((state, action, reward, next_state, done))\n",
        "                state = next_state\n",
        "                total_reward += reward\n",
        "                self.train_step += 1\n",
        "\n",
        "                if self.train_step > self.config[\"learning_starts\"] and self.train_step % self.config[\"train_frequency\"] == 0:\n",
        "                    self.agent.update(self.config[\"batch_size\"])\n",
        "\n",
        "                if self.train_step % self.config[\"target_update\"] == 0:\n",
        "                    self.agent.update_target()\n",
        "\n",
        "                if self.train_step > self.config[\"learning_starts\"]:\n",
        "                    self.epsilon = max(self.epsilon * self.config[\"epsilon_decay\"], self.config[\"epsilon_min\"])\n",
        "\n",
        "\n",
        "                if render:\n",
        "                    self.render_env(next_obs, screen)\n",
        "\n",
        "            episode_rewards.append(total_reward)\n",
        "            mean_score = np.mean(episode_rewards[-50:])  #mean over last 50\n",
        "\n",
        "            if (episode+1)%200 == 0:\n",
        "                print(f\"Episode {episode+1}, Reward: {total_reward}, Mean(50): {mean_score:.2f}, Epsilon: {self.epsilon:.3f}\")\n",
        "\n",
        "                if mean_score > self.best_mean_score:\n",
        "                  self.best_mean_score = mean_score\n",
        "                  torch.save(self.agent.q_net.state_dict(), \"model.pth\")\n",
        "                  print(f\"[Checkpoint] ✅ New best mean score: {mean_score:.2f} at episode {episode+1}. Model saved.\")\n",
        "\n",
        "            if render:\n",
        "                pygame.quit()\n",
        "\n",
        "        self.env.close()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OkxfGnw8_q_0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    random.seed(42)\n",
        "    np.random.seed(42)\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(42)\n",
        "\n",
        "    trainer = AtariTrainer(config)\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "KOjgPayQ_yE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b1083b8-d37c-4ddf-884f-b3cbb32003d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 200, Reward: 1.0, Mean(50): 2.00, Epsilon: 0.579\n",
            "[Checkpoint] ✅ New best mean score: 2.00 at episode 200. Model saved.\n",
            "Episode 400, Reward: 12.0, Mean(50): 8.46, Epsilon: 0.050\n",
            "[Checkpoint] ✅ New best mean score: 8.46 at episode 400. Model saved.\n",
            "Episode 600, Reward: 10.0, Mean(50): 10.80, Epsilon: 0.050\n",
            "[Checkpoint] ✅ New best mean score: 10.80 at episode 600. Model saved.\n",
            "Episode 800, Reward: 13.0, Mean(50): 11.78, Epsilon: 0.050\n",
            "[Checkpoint] ✅ New best mean score: 11.78 at episode 800. Model saved.\n"
          ]
        }
      ]
    }
  ]
}